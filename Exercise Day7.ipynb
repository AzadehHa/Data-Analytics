# Exercises Day 7- Group Exercises: Advanced
# Regression Analysis and Feature Selection

Perform regression analysis on a dataset with over 1000 data points and at least 30 features. Apply preprocessing,
build models, evaluate performance, and enhance the model using feature selection methods. Demonstrate
creativity with additional techniques or insights


dataset source:https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success

pip install ucimlrepo

import dataset:Predict Students' Dropout and Academic Success

+Instances
4424

+Features
36

from ucimlrepo import fetch_ucirepo

predict_students_dropout_and_academic_success = fetch_ucirepo(id=697)

X = predict_students_dropout_and_academic_success.data.features
y = predict_students_dropout_and_academic_success.data.targets

print("first 5 rows of X")
X.head()







print("first 5 rows of y")
y.head()


# **1. Preprocessing**
  1. Handle missing values and outliers.
  2. Scale features using standardization or normalization.
  3. Perform feature selection.

import pandas as pd


numerical_cols = X.select_dtypes(include=['number']).columns
for col in numerical_cols:
    if X[col].isnull().any():
        mean_val = X[col].mean()
        X[col].fillna(mean_val, inplace=True)

categorical_cols = X.select_dtypes(exclude=['number']).columns
for col in categorical_cols:
    if X[col].isnull().any():
        mode_val = X[col].mode()[0]
        X[col].fillna(mode_val, inplace=True)

if X.isnull().any().any():
    print("There are still missing values in the dataset.")
else:
    print("There are no missing values in the dataset.")

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
scaler = MinMaxScaler()
X.loc[:, numerical_cols] = scaler.fit_transform(X.loc[:, numerical_cols])
encoder = LabelEncoder()
y.loc[:, 'Target'] = encoder.fit_transform(y.loc[:, 'Target'])
scaler = MinMaxScaler()
y.loc[:, 'Target'] = scaler.fit_transform(y.loc[:, 'Target'].values.reshape(-1, 1))



Feature selection using correlation

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

target_column = 'Target'

correlations = X.corrwith(y[target_column])

# threshold
correlation_threshold = 0.1

selected_features = correlations[abs(correlations) > correlation_threshold].index

X_selected = X[selected_features]

X_train, X_test, y_train, y_test = train_test_split(X_selected, y[target_column], test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

print("\nSelected Features:")
print(selected_features)
X_selected.head()


an R-squared of 0.56 indicates that there's still a significant portion of the variance in student performance that's not explained by the model.
there is room for improvement.

#2. Build and Evaluate Models
 1. Train a Linear Regression model.
 2. Evaluate using R² Score andRMSE.
 3. PerformK-Fold Cross-Validation (e.g., 5-fold).

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

X_train, X_test, y_train, y_test = train_test_split(X_selected, y['Target'], test_size=0.2, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")



plt.figure(figsize=(8, 6))
sns.histplot(y_test, label='Actual', kde=True, color='blue')
sns.histplot(y_pred, label='Predicted', kde=True, color='red')
plt.xlabel("Target Values")
plt.ylabel("Frequency")
plt.title("Distribution of Actual vs. Predicted Target Values")
plt.legend()
plt.show()

from sklearn.model_selection import KFold
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# 15-fold cross-validation
kf = KFold(n_splits=15, shuffle=True, random_state=42)
mse_scores = []
r2_scores = []

for train_index, test_index in kf.split(X_selected):
    X_train, X_test = X_selected.iloc[train_index], X_selected.iloc[test_index]
    y_train, y_test = y['Target'].iloc[train_index], y['Target'].iloc[test_index]

    model = LinearRegression()
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    mse_scores.append(rmse)
    r2_scores.append(r2)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))  # 1 row, 2 columns

# Plot RMSE
axes[0].plot(range(1, len(mse_scores) + 1), mse_scores, marker='o', label='RMSE')
axes[0].set_xlabel('Fold')
axes[0].set_ylabel('RMSE')
axes[0].set_title('RMSE Across Folds')
axes[0].grid(True)

# Plot R-squared
axes[1].plot(range(1, len(r2_scores) + 1), r2_scores, marker='o', label='R-squared')
axes[1].set_xlabel('Fold')
axes[1].set_ylabel('R-squared')
axes[1].set_title('R-squared Across Folds')
axes[1].grid(True)

plt.tight_layout()
plt.show()




#3. Enhance the Model
 1. Experiment with feature selection techniques and analyze their impact.
 2. Visualize results (e.g., feature importance, residual plots).

 Wrapper Methods:
 Recursive Feature Elimination (RFE)

from sklearn.feature_selection import RFE

num_features_to_select = 10

rfe = RFE(estimator=model, n_features_to_select=num_features_to_select)

rfe = rfe.fit(X_selected, y['Target'])

selected_features_rfe = X_selected.columns[rfe.support_]

print("Selected Features using RFE:")
print(selected_features_rfe)

X_train_rfe, X_test_rfe, y_train_rfe, y_test_rfe = train_test_split(
    X_selected[selected_features_rfe], y['Target'], test_size=0.2, random_state=42
)

model_rfe = LinearRegression()
model_rfe.fit(X_train_rfe, y_train_rfe)
y_pred_rfe = model_rfe.predict(X_test_rfe)

mse_rfe = mean_squared_error(y_test_rfe, y_pred_rfe)
r2_rfe = r2_score(y_test_rfe, y_pred_rfe)

print(f"\nMean Squared Error (RFE): {mse_rfe}")
print(f"R-squared (RFE): {r2_rfe}")

importances = model_rfe.coef_
feature_importance = pd.Series(importances, index=selected_features_rfe)
plt.figure(figsize=(10, 6))
feature_importance.plot(kind='bar')
plt.title('Feature Importance (RFE)')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
X_selected[selected_features_rfe].head()

Residuals visualization:

Residual = Observed value – Predicted value



residuals = y_test_rfe - y_pred_rfe

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

sns.histplot(residuals, kde=True, ax=axes[0])
axes[0].set_xlabel("Residuals")
axes[0].set_ylabel("Frequency")
axes[0].set_title("Distribution of Residuals")


axes[1].scatter(y_pred_rfe,y_test_rfe)
axes[1].set_xlabel(" Predicted Values")
axes[1].set_ylabel("Actual values")
axes[1].set_title("Actual vs Predicted")

plt.show()

The center of the distribution seems to be around 0.This indicates that, on average, the model's predictions are neither consistently **overestimating** nor **underestimating** the actual values.
the presence of clusters in the second graph indicates that the model lack the capacity to capture the full range of variation in the actual values.
=> the model is limited


# 4. Creativity
   1. Try advanced models (e.g., Ridge, Lasso, Polynomial).
   2. Create insightful visualizations or optimize hyperparameters.
   3. Put the nice text justification on your shared colab


from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

X_train_rfe, X_test_rfe, y_train_rfe, y_test_rfe = train_test_split(
    X_selected[selected_features_rfe], y['Target'], test_size=0.2, random_state=42
)

# Ridge Regression
alpha = 0.7
ridge_model = Ridge(alpha=alpha)
ridge_model.fit(X_train_rfe, y_train_rfe)

y_pred_ridge = ridge_model.predict(X_test_rfe)

mse_ridge = mean_squared_error(y_test_rfe, y_pred_ridge)
r2_ridge = r2_score(y_test_rfe, y_pred_ridge)

print(f"\nMean Squared Error (Ridge): {mse_ridge}")
print(f"R-squared (Ridge): {r2_ridge}")

# Residuals visualization
residuals_ridge = y_test_rfe - y_pred_ridge

plt.figure(figsize=(10, 6))
plt.scatter(y_pred_ridge, residuals_ridge, label='Residuals', alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--', label='Zero Residual Line')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot for Ridge Regression")
plt.legend()
plt.grid(True)

# Add accuracy metric to the plot
accuracy_text = f"R-squared: {r2_ridge:.4f}"
plt.text(0.05, 0.95, accuracy_text, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')


plt.show()


from sklearn.linear_model import Lasso


# Lasso Regression
alpha = 0.01
lasso_model = Lasso(alpha=alpha)
lasso_model.fit(X_train_rfe, y_train_rfe)

y_pred_lasso = lasso_model.predict(X_test_rfe)

mse_lasso = mean_squared_error(y_test_rfe, y_pred_lasso)
r2_lasso = r2_score(y_test_rfe, y_pred_lasso)

print(f"\nMean Squared Error (Lasso): {mse_lasso}")
print(f"R-squared (Lasso): {r2_lasso}")

# Residuals visualization
residuals_lasso = y_test_rfe - y_pred_lasso

plt.figure(figsize=(10, 6))
plt.scatter(y_pred_lasso, residuals_lasso, label='Residuals', alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--', label='Zero Residual Line')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot for Lasso Regression")
plt.legend()
plt.grid(True)

# Accuracy metric
accuracy_text = f"R-squared: {r2_lasso:.4f}"
plt.text(0.05, 0.95, accuracy_text, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')

plt.show()


from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

degree = 2
poly_features = PolynomialFeatures(degree=degree, include_bias=False)

poly_model = make_pipeline(poly_features, LinearRegression())
poly_model.fit(X_train_rfe, y_train_rfe)

y_pred_poly = poly_model.predict(X_test_rfe)

mse_poly = mean_squared_error(y_test_rfe, y_pred_poly)
r2_poly = r2_score(y_test_rfe, y_pred_poly)

print(f"\nMean Squared Error (Polynomial): {mse_poly}")
print(f"R-squared (Polynomial): {r2_poly}")

# Residuals visualization
residuals_poly = y_test_rfe - y_pred_poly

plt.figure(figsize=(10, 6))
plt.scatter(y_pred_poly, residuals_poly, label='Residuals', alpha=0.7)
plt.axhline(y=0, color='red', linestyle='--', label='Zero Residual Line')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot for Polynomial Regression")
plt.legend()
plt.grid(True)

# Accuracy metric
accuracy_text = f"R-squared: {r2_poly:.4f}"
plt.text(0.05, 0.95, accuracy_text, transform=plt.gca().transAxes, fontsize=12, verticalalignment='top')

plt.show()

from sklearn.model_selection import cross_val_score
from sklearn.metrics import make_scorer, mean_squared_error
import numpy as np

def cross_validation(model, X, y, cv=5):
    r2_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')

    rmse_scorer = make_scorer(mean_squared_error, squared=False)
    rmse_scores = cross_val_score(model, X, y, cv=cv, scoring=rmse_scorer)

    return r2_scores, rmse_scores

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=0.01),
    'Lasso Regression': Lasso(alpha=0.01),
    'Polynomial Regression': make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())
}

for name, model in models.items():
    r2_scores, rmse_scores = cross_validation(model, X_selected[selected_features_rfe], y['Target'])

    print(f"{name} Cross-Validation R^2 scores: {r2_scores}")
    print(f"{name} Average R^2: {r2_scores.mean()}")
    print(f"{name} Standard Deviation (R^2): {r2_scores.std()}")

    print(f"{name} Cross-Validation RMSE scores: {rmse_scores}")
    print(f"{name} Average RMSE: {rmse_scores.mean()}")
    print(f"{name} Standard Deviation (RMSE): {rmse_scores.std()}")

    print("-"*30)

import matplotlib.pyplot as plt

models = {
    'Ridge Regression': {'r2': [0.58002947, 0.60566937, 0.56881431, 0.55644649, 0.57398358], 'rmse': [0.28210269, 0.27819999, 0.29220887, 0.2966646, 0.29293612]},
    'Lasso Regression': {'r2': [0.44437297, 0.5085787, 0.44353226, 0.44254164, 0.45039882], 'rmse': [0.32448117, 0.31056596, 0.33195654, 0.33258215, 0.33272358]},
    'Polynomial Regression': {'r2': [0.61780181, 0.64482831, 0.59843937, 0.59859768, 0.61544464], 'rmse': [0.26911762, 0.2640256, 0.281992, 0.28221666, 0.27831665]}
}

fig, axes = plt.subplots(1, 2, figsize=(20, 10))

# Plot R-squared
for name, metrics in models.items():
    axes[0].plot(range(1, len(metrics['r2']) + 1), metrics['r2'], label=name, marker='o', linestyle="--")

axes[0].set_title("R-squared Across Folds")
axes[0].set_xlabel("Fold")
axes[0].set_ylabel("R-squared")
axes[0].legend()
axes[0].grid(True)

# Plot RMSE values
for name, metrics in models.items():
    axes[1].plot(range(1, len(metrics['rmse']) + 1), metrics['rmse'], label=name, marker='x', linestyle="--")

axes[1].set_title("RMSE Across Folds")
axes[1].set_xlabel("Fold")
axes[1].set_ylabel("RMSE")
axes[1].legend()
axes[1].grid(True)

plt.tight_layout()
plt.show()

Polynomial Regression Performs Best in R²:

Polynomial Regression consistently achieves the highest R² values across all folds, capturing complex patterns in the data. However, its slightly larger variability in R² might indicate overfitting to certain folds.
Ridge Regression Offers Stability:

Ridge Regression performs similarly to Linear Regression but adds stability through regularization, ensuring it handles multicollinearity effectively.
Lasso Regression Underperforms:

Lasso Regression has the lowest R² and slightly higher RMSE. This might be due to excessive regularization or poor suitability for the dataset.
Linear Regression as a Baseline:

Linear Regression performs well with minimal fluctuations in both R² and RMSE, making it a reliable baseline model.


import matplotlib.pyplot as plt

models = {
    'Ridge Regression': {'r2': [0.58002947, 0.60566937, 0.56881431, 0.55644649, 0.57398358], 'rmse': [0.28210269, 0.27819999, 0.29220887, 0.2966646, 0.29293612]},
    'Lasso Regression': {'r2': [0.44437297, 0.5085787, 0.44353226, 0.44254164, 0.45039882], 'rmse': [0.32448117, 0.31056596, 0.33195654, 0.33258215, 0.33272358]},
    'Polynomial Regression': {'r2': [0.61780181, 0.64482831, 0.59843937, 0.59859768, 0.61544464], 'rmse': [0.26911762, 0.2640256, 0.281992, 0.28221666, 0.27831665]}
}

fig, axes = plt.subplots(1, 2, figsize=(20, 10))

r2_means = [np.mean(metrics['r2']) for _, metrics in models.items()]
axes[0].bar(models.keys(), r2_means, color=['skyblue', 'lightcoral', 'lightgreen'])

axes[0].set_title("Mean R-squared of all models")
axes[0].set_xlabel("Models")
axes[0].set_ylabel("Mean R-squared")
axes[0].tick_params(axis='x', rotation=45)
axes[0].grid(True)

for i, v in enumerate(r2_means):
    axes[0].text(i, v + 0.001, f"{v:.4f}", ha='center', va='bottom')

rmse_means = [np.mean(metrics['rmse']) for _, metrics in models.items()]
axes[1].bar(models.keys(), rmse_means, color=['skyblue', 'lightcoral', 'lightgreen'])

axes[1].set_title("Mean RMSE of all models")
axes[1].set_xlabel("Models")
axes[1].set_ylabel("Mean RMSE")
axes[1].tick_params(axis='x', rotation=45)
axes[1].grid(True)

for i, v in enumerate(rmse_means):
    axes[1].text(i, v + 0.001, f"{v:.4f}", ha='center', va='bottom')


plt.tight_layout()
plt.show()

 while the polynomial model in this case achieves the highest R-squared value, it's crucial to evaluate its performance in terms of generalization and avoid overfitting. Regularization, model selection techniques, and cross-validation are essential tools to make informed decisions about model complexity.

 we will use f-test.

import statsmodels.api as sm
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

def perform_f_test(X_train, y_train, X_test, y_test, degree):
    poly = PolynomialFeatures(degree=degree, include_bias=False)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = sm.OLS(y_train, X_train_poly).fit()

    f_statistic = model.fvalue
    p_value = model.f_pvalue
    print(f"F-statistic for polynomial degree {degree}: {f_statistic}")
    print(f"P-value for polynomial degree {degree}: {p_value}")



    return f_statistic, p_value

f_statistic, p_value = perform_f_test(X_train_rfe, y_train_rfe, X_test_rfe, y_test_rfe, degree=2)

With a highly significant F-statistic for the polynomial degree 2, the polynomial model appears to be a strong choice. This suggests that the polynomial terms contribute significantly to the model's ability to explain the variance in the data.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('application_record.csv')

# Drop rows with missing target values
data = data.dropna(subset=['NAME_INCOME_TYPE'])

# Define target (y) and features (X)
X = data.drop(columns=['NAME_INCOME_TYPE'])
y = data['NAME_INCOME_TYPE']

# Encode categorical target
le = LabelEncoder()
y = le.fit_transform(y)

# Encode categorical features
categorical_features = X.select_dtypes(include=['object']).columns
for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature])

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Initialize models
dt_model = DecisionTreeClassifier(random_state=42)
rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
gbt_model = GradientBoostingClassifier(random_state=42)

# Train models
dt_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
gbt_model.fit(X_train, y_train)

# Make predictions
dt_preds = dt_model.predict(X_test)
rf_preds = rf_model.predict(X_test)
gbt_preds = gbt_model.predict(X_test)

# Evaluate models
print("Decision Tree Classifier:")
print(classification_report(y_test, dt_preds))
print("\nRandom Forest Classifier:")
print(classification_report(y_test, rf_preds))
print("\nGradient Boosted Trees Classifier:")
print(classification_report(y_test, gbt_preds))

# Compare Accuracy
dt_acc = accuracy_score(y_test, dt_preds)
rf_acc = accuracy_score(y_test, rf_preds)
gbt_acc = accuracy_score(y_test, gbt_preds)

print(f"Accuracy Scores:\nDecision Tree: {dt_acc:.4f}\nRandom Forest: {rf_acc:.4f}\nGradient Boosted Trees: {gbt_acc:.4f}")

# Plot Feature Importance for Random Forest
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()


Decision Tree Classifier:
              precision    recall  f1-score   support

           0       0.93      0.93      0.93     30148
           1       1.00      1.00      1.00     22631
           2       0.93      0.94      0.93     10773
           3       0.83      0.71      0.77         7
           4       0.97      0.96      0.97     68009

    accuracy                           0.96    131568
   macro avg       0.93      0.91      0.92    131568
weighted avg       0.96      0.96      0.96    131568


Random Forest Classifier:
              precision    recall  f1-score   support

           0       0.98      0.93      0.96     30148
           1       1.00      1.00      1.00     22631
           2       0.99      0.94      0.96     10773
           3       1.00      0.71      0.83         7
           4       0.96      0.99      0.98     68009

    accuracy                           0.98    131568
   macro avg       0.99      0.92      0.95    131568
weighted avg       0.98      0.98      0.98    131568


Gradient Boosted Trees Classifier:
              precision    recall  f1-score   support

           0       0.55      0.13      0.22     30148
           1       1.00      1.00      1.00     22631
           2       0.49      0.09      0.16     10773
           3       0.45      0.71      0.56         7
           4       0.65      0.95      0.77     68009

    accuracy                           0.70    131568
   macro avg       0.63      0.58      0.54    131568
weighted avg       0.67      0.70      0.63    131568

Accuracy Scores:
Decision Tree: 0.9604
Random Forest: 0.9755
Gradient Boosted Trees: 0.7012
 
Therefore, Random Forest is the best-performing model in its current configuration, achieving 97.55% accuracy.

------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('application_record.csv')

# Drop rows with missing target values
data = data.dropna(subset=['NAME_INCOME_TYPE'])

# Define target (y) and features (X)
X = data.drop(columns=['NAME_INCOME_TYPE'])
y = data['NAME_INCOME_TYPE']

# Create a LabelEncoder specifically for the target variable
target_encoder = LabelEncoder()  # Create a separate encoder for the target
y = target_encoder.fit_transform(y)

# Encode categorical features
categorical_features = X.select_dtypes(include=['object']).columns
for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature])

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Initialize KNN model with n_neighbors=3
knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(X_train, y_train)

# Make predictions
y_pred = knn_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=target_encoder.classes_))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_encoder.classes_, yticklabels=target_encoder.classes_)
plt.title('Confusion Matrix for KNN (n_neighbors=3)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

Accuracy: 0.9224

Classification Report:
                      precision    recall  f1-score   support

Commercial associate       0.87      0.86      0.87     20236
           Pensioner       1.00      1.00      1.00     15093
       State servant       0.87      0.85      0.86      7141
             Student       0.75      0.75      0.75         4
             Working       0.93      0.93      0.93     45238

            accuracy                           0.92     87712
           macro avg       0.88      0.88      0.88     87712
        weighted avg       0.92      0.92      0.92     87712

If we change the n_neighbors to 5, the result will be like below:

Accuracy: 0.8877

Classification Report:
                      precision    recall  f1-score   support

Commercial associate       0.82      0.79      0.80     20236
           Pensioner       1.00      1.00      1.00     15093
       State servant       0.80      0.78      0.79      7141
             Student       0.60      0.75      0.67         4
             Working       0.90      0.91      0.90     45238

            accuracy                           0.89     87712
           macro avg       0.82      0.85      0.83     87712
        weighted avg       0.89      0.89      0.89     87712

So, we can see that the n_neighbors=3 is better because the Accuracy is 0.9224.


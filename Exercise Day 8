import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('application_record.csv')

# Drop rows with missing target values
data = data.dropna(subset=['NAME_INCOME_TYPE'])

# Define target (y) and features (X)
X = data.drop(columns=['NAME_INCOME_TYPE'])
y = data['NAME_INCOME_TYPE']

# Encode categorical target
le = LabelEncoder()
y = le.fit_transform(y)

# Encode categorical features
categorical_features = X.select_dtypes(include=['object']).columns
for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature])

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Initialize models
dt_model = DecisionTreeClassifier(random_state=42)
rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)
gbt_model = GradientBoostingClassifier(random_state=42)

# Train models
dt_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)
gbt_model.fit(X_train, y_train)

# Make predictions
dt_preds = dt_model.predict(X_test)
rf_preds = rf_model.predict(X_test)
gbt_preds = gbt_model.predict(X_test)

# Evaluate models
print("Decision Tree Classifier:")
print(classification_report(y_test, dt_preds))
print("\nRandom Forest Classifier:")
print(classification_report(y_test, rf_preds))
print("\nGradient Boosted Trees Classifier:")
print(classification_report(y_test, gbt_preds))

# Compare Accuracy
dt_acc = accuracy_score(y_test, dt_preds)
rf_acc = accuracy_score(y_test, rf_preds)
gbt_acc = accuracy_score(y_test, gbt_preds)

print(f"Accuracy Scores:\nDecision Tree: {dt_acc:.4f}\nRandom Forest: {rf_acc:.4f}\nGradient Boosted Trees: {gbt_acc:.4f}")

# Plot Feature Importance for Random Forest
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()


Decision Tree Classifier:
              precision    recall  f1-score   support

           0       0.93      0.93      0.93     30148
           1       1.00      1.00      1.00     22631
           2       0.93      0.94      0.93     10773
           3       0.83      0.71      0.77         7
           4       0.97      0.96      0.97     68009

    accuracy                           0.96    131568
   macro avg       0.93      0.91      0.92    131568
weighted avg       0.96      0.96      0.96    131568


Random Forest Classifier:
              precision    recall  f1-score   support

           0       0.98      0.93      0.96     30148
           1       1.00      1.00      1.00     22631
           2       0.99      0.94      0.96     10773
           3       1.00      0.71      0.83         7
           4       0.96      0.99      0.98     68009

    accuracy                           0.98    131568
   macro avg       0.99      0.92      0.95    131568
weighted avg       0.98      0.98      0.98    131568


Gradient Boosted Trees Classifier:
              precision    recall  f1-score   support

           0       0.55      0.13      0.22     30148
           1       1.00      1.00      1.00     22631
           2       0.49      0.09      0.16     10773
           3       0.45      0.71      0.56         7
           4       0.65      0.95      0.77     68009

    accuracy                           0.70    131568
   macro avg       0.63      0.58      0.54    131568
weighted avg       0.67      0.70      0.63    131568

Accuracy Scores:
Decision Tree: 0.9604
Random Forest: 0.9755
Gradient Boosted Trees: 0.7012
 
Therefore, Random Forest is the best-performing model in its current configuration, achieving 97.55% accuracy.

------------------------------------------------------------------------------------------------------------------------



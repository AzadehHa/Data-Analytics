from google.colab import files
uploded=files.upload()
import pandas as pd
data = pd.read_csv('application_record.csv')
print(data.head())

print(data.isnull().sum())

import pandas as pd


data = pd.read_csv('application_record.csv')  

# Check for missing values
print("Missing values before imputation:")
print(data.isnull().sum())

# Handling missing values
# Numerical columns: Impute with mean or median
numerical_columns = ['ID', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS']
for col in numerical_columns:
    data[col] = data[col].fillna(data[col].median())  # Use median for numerical columns

# Categorical columns: Impute with mode
categorical_columns = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'OCCUPATION_TYPE']
for col in categorical_columns:
    data[col] = data[col].fillna(data[col].mode()[0])  # Use mode for categorical columns

# Check for missing values after imputation
print("\nMissing values after imputation:")
print(data.isnull().sum())

# Save the cleaned dataset
data.to_csv('application_record_cleaned.csv', index=False)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


data = pd.read_csv('application_record.csv') 


# Plot histograms for understanding about the nooise
numerical_columns = ['ID', 'CNT_CHILDREN', 'AMT_INCOME_TOTAL', 'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE', 'FLAG_EMAIL', 'CNT_FAM_MEMBERS']
for col in numerical_columns:
    plt.figure(figsize=(6, 4))
    sns.histplot(data[col], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.show()

from scipy.stats import zscore

from google.colab import files
uploded=files.upload()
import pandas as pd
data = pd.read_csv('application_record.csv')

# Compute Z-scores for the 'AMT_INCOME_TOTAL' column
data['Z_Score_AMT_INCOME_TOTAL'] = zscore(data['AMT_INCOME_TOTAL'])

# Identify outliers based on Z-score threshold (e.g., |Z| > 3)
outliers = data[(data['Z_Score_AMT_INCOME_TOTAL'] > 3) | (data['Z_Score_AMT_INCOME_TOTAL'] < -3)]

print("Outliers detected in 'AMT_INCOME_TOTAL':")
print(outliers)

from google.colab import files
import pandas as pd
import numpy as np
data = pd.read_csv('application_record.csv')

# Apply log transformation to 'AMT_INCOME_TOTAL'
data['Transformed_AMT_INCOME_TOTAL'] = np.log1p(data['AMT_INCOME_TOTAL'])  # log1p avoids log(0)

# Plot the transformed data to verify changes
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(6, 4))
sns.histplot(data['Transformed_AMT_INCOME_TOTAL'], kde=True, bins=30, color='blue')
plt.title('Distribution of Log-Transformed AMT_INCOME_TOTAL')
plt.xlabel('Log(AMT_INCOME_TOTAL)')
plt.ylabel('Frequency')
plt.show()


import pandas as pd
import numpy as np
from sklearn.feature_selection import mutual_info_regression # Changed to mutual_info_regression
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
uploded=files.upload()
data = pd.read_csv('application_record.csv')

# Separate features and target
X = data.drop(columns=['AMT_INCOME_TOTAL']) 
y = data['AMT_INCOME_TOTAL']                

# Separate numerical and categorical features
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns
categorical_features = X.select_dtypes(include=['object', 'category']).columns

# Compute correlations between numerical features and target
correlations = data[numerical_features].corrwith(y)

# Display the correlations
correlation_df = correlations.sort_values(ascending=False).reset_index()
correlation_df.columns = ['Feature', 'Correlation with AMT_INCOME_TOTAL']
print(correlation_df)

# Compute correlation matrix
correlation_matrix = data[numerical_features].corr()

# Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', cbar=True)
plt.title('Correlation Matrix')
plt.show()

selected_features_correlation = correlation_df[correlation_df['Correlation with AMT_INCOME_TOTAL'].abs() > 0.2]['Feature']
print("Selected features based on correlation:", selected_features_correlation.tolist())

# Compute mutual information scores
# Changed to mutual_info_regression for continuous target
mi_scores = mutual_info_regression(X[numerical_features], y, random_state=42) 

# Create a DataFrame for visualization
mi_df = pd.DataFrame({'Feature': numerical_features, 'Mutual Information': mi_scores})
mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)
print(mi_df)

# Bar plot of mutual information scores
plt.figure(figsize=(10, 6))
sns.barplot(x='Mutual Information', y='Feature', data=mi_df, palette='viridis')
plt.title('Mutual Information Scores')
plt.show()

# Set thresholds
correlation_threshold = 0.1
mi_threshold = 0.005

# Select features from correlation
correlation_features = correlation_df[correlation_df['Correlation with AMT_INCOME_TOTAL'].abs() > correlation_threshold]['Feature']

# Select features from mutual information
mutual_information_features = mi_df[mi_df['Mutual Information'] > mi_threshold]['Feature']

# Combine the two
final_selected_features = set(correlation_features).intersection(set(mutual_information_features))
print("Final Selected Features:", final_selected_features)
-------------------------------------------------------
import pandas as pd
import numpy as np
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from google.colab import files
from sklearn.preprocessing import LabelEncoder 
import matplotlib.pyplot as plt

uploded=files.upload()
data = pd.read_csv('application_record.csv')

# Assuming your target variable
X = data.drop(columns=['AMT_INCOME_TOTAL'])  
y = data['AMT_INCOME_TOTAL']

# Convert categorical features to numerical using Label Encoding
categorical_features = X.select_dtypes(include=['object']).columns # Select object type columns
for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature]) # Encode categorical features

# Split the dataset for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Define the model
model = RandomForestRegressor(random_state=42)

# Define RFE with the desired number of features to select
n_features_to_select = 5  # Choose how many features you want to keep
rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)

# Fit RFE to the training data
rfe.fit(X_train, y_train)

# Get the rankings of features
feature_ranking = pd.DataFrame({
    'Feature': X_train.columns,
    'Ranking': rfe.ranking_
}).sort_values(by='Ranking')

# Display selected features
selected_features = feature_ranking[feature_ranking['Ranking'] == 1]['Feature']
print("Selected Features:")
print(selected_features)


# Plot feature rankings
plt.figure(figsize=(10, 6))
plt.barh(feature_ranking['Feature'], feature_ranking['Ranking'], color='skyblue')
plt.xlabel('Ranking (1 = Selected)')
plt.ylabel('Features')
plt.title('Feature Rankings by RFE')
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.show()

# Use only selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Train a model using selected features
model_selected = RandomForestRegressor(random_state=42)
model_selected.fit(X_train_selected, y_train)

# Evaluate performance
print("Model Performance with Selected Features:")
print("Training Accuracy:", model_selected.score(X_train_selected, y_train))
print("Testing Accuracy:", model_selected.score(X_test_selected, y_test))

from sklearn.metrics import accuracy_score

# Test different numbers of features
results = []
for n_features in range(1, X_train.shape[1] + 1):
    rfe = RFE(estimator=model, n_features_to_select=n_features)
    rfe.fit(X_train, y_train)
    selected_features = X_train.columns[rfe.support_]
    model_selected = RandomForestRegressor(random_state=42)
    model_selected.fit(X_train[selected_features], y_train)
    test_accuracy = model_selected.score(X_test[selected_features], y_test)
    results.append((n_features, test_accuracy))

# Plot results
results_df = pd.DataFrame(results, columns=['n_features', 'test_accuracy'])
plt.figure(figsize=(10, 6))
plt.plot(results_df['n_features'], results_df['test_accuracy'], marker='o')
plt.xlabel('Number of Features Selected')
plt.ylabel('Test Accuracy')
plt.title('Feature Selection Performance')
plt.show()
----------------------------------------------------------------------------
import pandas as pd
import numpy as np
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import matplotlib.pyplot as plt

# Load the dataset
data = pd.read_csv('application_record.csv')

# Drop missing values in the target column
data = data.dropna(subset=['AMT_INCOME_TOTAL'])

# Define features (X) and target (y)
X = data.drop(columns=['AMT_INCOME_TOTAL'])
y = data['AMT_INCOME_TOTAL']

# Encode categorical features
categorical_features = X.select_dtypes(include=['object']).columns
for feature in categorical_features:
    le = LabelEncoder()
    X[feature] = le.fit_transform(X[feature])

# Standardize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Apply Lasso Regression
alpha_value = 0.02  
lasso = Lasso(alpha=alpha_value, random_state=42)
lasso.fit(X_train, y_train)

# Extract non-zero coefficients (selected features)
selected_features = np.array(X.columns)[lasso.coef_ != 0]
dropped_features = np.array(X.columns)[lasso.coef_ == 0]

print(f"Selected Features ({len(selected_features)}):", selected_features)
print(f"Dropped Features ({len(dropped_features)}):", dropped_features)

# Feature importance (absolute coefficient values)
lasso_importances = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lasso.coef_
}).sort_values(by='Coefficient', key=abs, ascending=False)

print("\nFeature Importance from Lasso Regression:")
print(lasso_importances)

# Plot Lasso Coefficients
plt.figure(figsize=(10, 6))
plt.barh(lasso_importances['Feature'], lasso_importances['Coefficient'], color='lightgreen')
plt.xlabel('Coefficient Value')
plt.ylabel('Features')
plt.title('Feature Importance from Lasso Regression')
plt.gca().invert_yaxis()
plt.show()
